One approach popular for requirements analysis is Use Case analysis.
However, Charles Babbage had already written his first program for the Analytical Engine in 1837.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
 A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
 Code-breaking algorithms have also existed for centuries.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
 Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.

Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute.
Normally the first step in debugging is to attempt to reproduce the problem.
Also, specific user environment and usage history can make it difficult to reproduce the problem.
Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.
 Programmable devices have existed for centuries.