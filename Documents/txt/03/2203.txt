This can be a non-trivial task, for example as with parallel processes or some unusual software bugs.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
 Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line.
In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages.
Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit.
 It is very difficult to determine what are the most popular modern programming languages.
 Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications.
Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.
Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.
There exist a lot of different approaches for each of those tasks.
 After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug.
It affects the aspects of quality above, including portability, usability and most importantly maintainability.

For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
There are many approaches to the Software development process.