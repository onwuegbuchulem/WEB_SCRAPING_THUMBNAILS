It is usually easier to code in "high-level" languages than in "low-level" ones.
He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.
In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages.
However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.

The first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'.
 After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug.
 Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.
Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute.
 Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code.
Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages.
 Whatever the approach to development may be, the final program must satisfy some fundamental properties.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
 It is very difficult to determine what are the most popular modern programming languages.