 After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug.
 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
 Programmable devices have existed for centuries.
 Computer programmers are those who write computer software.
The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference.
Normally the first step in debugging is to attempt to reproduce the problem.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years.
 Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code.
 Programs were mostly entered using punched cards or paper tape.
 Allen Downey, in his book How To Think Like A Computer Scientist, writes:
 Many computer languages provide a mechanism to call functions provided by shared libraries.
 A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
 Different programming languages support different styles of programming (called programming paradigms).
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.