 These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics.
FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed—in particular, COBOL aimed at commercial data processing, and Lisp for computer research.
Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages.
While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se.
 Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code.
Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists.
Many applications use a mix of several languages in their construction and use.
 A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
 Computer programmers are those who write computer software.
 Programmable devices have existed for centuries.
It affects the aspects of quality above, including portability, usability and most importantly maintainability.
 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.