Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years.
 Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line.
 Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
 Following a consistent programming style often helps readability.
Ideally, the programming language best suited for the task at hand will be selected.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.
However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.
 A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages.