
 Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages.
 It is very difficult to determine what are the most popular modern programming languages.
FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developedâ€”in particular, COBOL aimed at commercial data processing, and Lisp for computer research.
Ideally, the programming language best suited for the task at hand will be selected.
 A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
Many applications use a mix of several languages in their construction and use.
 Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code.
Unreadable code often leads to bugs, inefficiencies, and duplicated code.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.
Techniques like Code refactoring can enhance readability.
There exist a lot of different approaches for each of those tasks.
 The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging).