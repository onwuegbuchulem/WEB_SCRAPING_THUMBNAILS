For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
 Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation.
He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
 The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems.
In 1801, the Jacquard loom could produce entirely different weaves by changing the "program" â€“ a series of pasteboard cards with holes punched in them.
Normally the first step in debugging is to attempt to reproduce the problem.
Unreadable code often leads to bugs, inefficiencies, and duplicated code.
Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists.
 Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications.
 Allen Downey, in his book How To Think Like A Computer Scientist, writes:
 Many computer languages provide a mechanism to call functions provided by shared libraries.
 Different programming languages support different styles of programming (called programming paradigms).
It is usually easier to code in "high-level" languages than in "low-level" ones.
 Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.