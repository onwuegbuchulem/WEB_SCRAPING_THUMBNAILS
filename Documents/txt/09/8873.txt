 Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.
They are the building blocks for all software, from the simplest applications to the most sophisticated ones.
 These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics.
One approach popular for requirements analysis is Use Case analysis.
Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists.
 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages.
However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.
 A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
 Following a consistent programming style often helps readability.
It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Scripting and breakpointing is also part of this process.