 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
 Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
Use of a static code analysis tool can help detect some possible problems.
Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers.
 Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation.
 Allen Downey, in his book How To Think Like A Computer Scientist, writes:
 Many computer languages provide a mechanism to call functions provided by shared libraries.
 Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users.
Ideally, the programming language best suited for the task at hand will be selected.
Many applications use a mix of several languages in their construction and use.
 Following a consistent programming style often helps readability.
However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.

Unreadable code often leads to bugs, inefficiencies, and duplicated code.
By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers.