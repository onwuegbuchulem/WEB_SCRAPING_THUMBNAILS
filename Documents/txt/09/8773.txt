 Allen Downey, in his book How To Think Like A Computer Scientist, writes:
 Many computer languages provide a mechanism to call functions provided by shared libraries.
 These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics.
They are the building blocks for all software, from the simplest applications to the most sophisticated ones.

 Following a consistent programming style often helps readability.
Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists.
By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers.
Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
Unreadable code often leads to bugs, inefficiencies, and duplicated code.
 Programmable devices have existed for centuries.
One approach popular for requirements analysis is Use Case analysis.
 Whatever the approach to development may be, the final program must satisfy some fundamental properties.
There exist a lot of different approaches for each of those tasks.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.