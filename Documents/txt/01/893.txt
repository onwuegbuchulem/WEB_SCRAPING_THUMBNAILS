Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
One approach popular for requirements analysis is Use Case analysis.
Techniques like Code refactoring can enhance readability.
 Programmable devices have existed for centuries.
Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.
By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages.

Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.
However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
Normally the first step in debugging is to attempt to reproduce the problem.
The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference.