 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
This can be a non-trivial task, for example as with parallel processes or some unusual software bugs.
 Programs were mostly entered using punched cards or paper tape.
Normally the first step in debugging is to attempt to reproduce the problem.
Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.
It is usually easier to code in "high-level" languages than in "low-level" ones.
 Different programming languages support different styles of programming (called programming paradigms).
However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
 Computer programmers are those who write computer software.
 It is very difficult to determine what are the most popular modern programming languages.
Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.
Use of a static code analysis tool can help detect some possible problems.