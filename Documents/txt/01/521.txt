Many applications use a mix of several languages in their construction and use.
One approach popular for requirements analysis is Use Case analysis.
 Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation.
Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly.
 Whatever the approach to development may be, the final program must satisfy some fundamental properties.
Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years.
 Programmable devices have existed for centuries.
 Code-breaking algorithms have also existed for centuries.
In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages.

 Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.
Scripting and breakpointing is also part of this process.
 Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.