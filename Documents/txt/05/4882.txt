By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers.
 Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display.
While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se.
Also, specific user environment and usage history can make it difficult to reproduce the problem.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
Their jobs usually involve:
 Although programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.
 Following a consistent programming style often helps readability.
FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed—in particular, COBOL aimed at commercial data processing, and Lisp for computer research.
However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.
Scripting and breakpointing is also part of this process.
 Programmable devices have existed for centuries.
 Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA).
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.