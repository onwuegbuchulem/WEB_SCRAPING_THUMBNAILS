
The first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'.
However, Charles Babbage had already written his first program for the Analytical Engine in 1837.
While these are sometimes considered programming, often the term software development is used for this larger overall process â€“ with the terms programming, implementation, and coding reserved for the writing and editing of code per se.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
 Code-breaking algorithms have also existed for centuries.
 Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code.
It is usually easier to code in "high-level" languages than in "low-level" ones.
 Programmable devices have existed for centuries.
 New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit.
 Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line.
 Computer programmers are those who write computer software.
 After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug.
This can be a non-trivial task, for example as with parallel processes or some unusual software bugs.
Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.