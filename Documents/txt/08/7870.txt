

The first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'.
Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers.
Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses.
Normally the first step in debugging is to attempt to reproduce the problem.
It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.
As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the Book of Ingenious Devices.
It is usually easier to code in "high-level" languages than in "low-level" ones.
Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.
When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
 Following a consistent programming style often helps readability.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
This can be a non-trivial task, for example as with parallel processes or some unusual software bugs.
He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.