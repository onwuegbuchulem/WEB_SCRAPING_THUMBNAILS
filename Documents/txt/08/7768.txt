 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
Normally the first step in debugging is to attempt to reproduce the problem.
The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference.
Also, specific user environment and usage history can make it difficult to reproduce the problem.
Use of a static code analysis tool can help detect some possible problems.

The first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'.
One approach popular for requirements analysis is Use Case analysis.
It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.
By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers.
Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.