 The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems.
 Programmable devices have existed for centuries.
Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers.
This can be a non-trivial task, for example as with parallel processes or some unusual software bugs.

 Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
Normally the first step in debugging is to attempt to reproduce the problem.
There are many approaches to the Software development process.
However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.
Also, specific user environment and usage history can make it difficult to reproduce the problem.
 Whatever the approach to development may be, the final program must satisfy some fundamental properties.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
 Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation.
However, readability is more than just programming style.