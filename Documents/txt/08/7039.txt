Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit.
 Code-breaking algorithms have also existed for centuries.
Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers.
However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
Ideally, the programming language best suited for the task at hand will be selected.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.
 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
 Computer programmers are those who write computer software.
 Whatever the approach to development may be, the final program must satisfy some fundamental properties.
 After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug.
Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.
However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.
 Following a consistent programming style often helps readability.