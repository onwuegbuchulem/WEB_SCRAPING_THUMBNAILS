Techniques like Code refactoring can enhance readability.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists.
 Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications.
Ideally, the programming language best suited for the task at hand will be selected.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
There are many approaches to the Software development process.
 These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics.
When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.
It is usually easier to code in "high-level" languages than in "low-level" ones.
Normally the first step in debugging is to attempt to reproduce the problem.
One approach popular for requirements analysis is Use Case analysis.
This can be a non-trivial task, for example as with parallel processes or some unusual software bugs.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.
 Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation.