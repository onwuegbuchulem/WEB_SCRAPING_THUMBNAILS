Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers.
He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.
 Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users.

Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.
In 1801, the Jacquard loom could produce entirely different weaves by changing the "program" â€“ a series of pasteboard cards with holes punched in them.
Normally the first step in debugging is to attempt to reproduce the problem.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
Some of these factors include:
 The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.
 Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
Many applications use a mix of several languages in their construction and use.
Use of a static code analysis tool can help detect some possible problems.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.