Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.
 Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.
Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.
 The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems.
There are many approaches to the Software development process.
However, readability is more than just programming style.
Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
Normally the first step in debugging is to attempt to reproduce the problem.
 Computer programmers are those who write computer software.
 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
 Allen Downey, in his book How To Think Like A Computer Scientist, writes:
 Many computer languages provide a mechanism to call functions provided by shared libraries.