Unreadable code often leads to bugs, inefficiencies, and duplicated code.
 These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics.
Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute.
 New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
 Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code.
Many applications use a mix of several languages in their construction and use.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
 Code-breaking algorithms have also existed for centuries.

The first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'.
Normally the first step in debugging is to attempt to reproduce the problem.
 Different programming languages support different styles of programming (called programming paradigms).
One approach popular for requirements analysis is Use Case analysis.
Techniques like Code refactoring can enhance readability.
 Programmable devices have existed for centuries.
 Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications.