For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
However, readability is more than just programming style.
 Computer programmers are those who write computer software.

Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
Unreadable code often leads to bugs, inefficiencies, and duplicated code.
Scripting and breakpointing is also part of this process.
Normally the first step in debugging is to attempt to reproduce the problem.
 Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display.
One approach popular for requirements analysis is Use Case analysis.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.
This can be a non-trivial task, for example as with parallel processes or some unusual software bugs.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
 Following a consistent programming style often helps readability.
Use of a static code analysis tool can help detect some possible problems.