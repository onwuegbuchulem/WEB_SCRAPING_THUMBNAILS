Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
 In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form.
 The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems.
Unreadable code often leads to bugs, inefficiencies, and duplicated code.
He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.
The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference.
Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
 Allen Downey, in his book How To Think Like A Computer Scientist, writes:
 Many computer languages provide a mechanism to call functions provided by shared libraries.
There exist a lot of different approaches for each of those tasks.
It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.
Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses.
 Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line.
 Programmable devices have existed for centuries.
Many applications use a mix of several languages in their construction and use.
Normally the first step in debugging is to attempt to reproduce the problem.