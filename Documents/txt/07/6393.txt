 Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation.
Normally the first step in debugging is to attempt to reproduce the problem.
It is usually easier to code in "high-level" languages than in "low-level" ones.
For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
 Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line.
 Programs were mostly entered using punched cards or paper tape.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
It affects the aspects of quality above, including portability, usability and most importantly maintainability.
 Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
 Different programming languages support different styles of programming (called programming paradigms).
 Following a consistent programming style often helps readability.
Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit.
 Code-breaking algorithms have also existed for centuries.