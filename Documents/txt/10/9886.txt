
 Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks.
A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.
There exist a lot of different approaches for each of those tasks.
Many applications use a mix of several languages in their construction and use.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
Ideally, the programming language best suited for the task at hand will be selected.
 The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging).
Normally the first step in debugging is to attempt to reproduce the problem.
Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.
 Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code.
 Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA).
Techniques like Code refactoring can enhance readability.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
In 1801, the Jacquard loom could produce entirely different weaves by changing the "program" â€“ a series of pasteboard cards with holes punched in them.

The first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'.