Normally the first step in debugging is to attempt to reproduce the problem.
For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software.
 High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware.
 Different programming languages support different styles of programming (called programming paradigms).
When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.
By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers.
In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where a musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams.
 Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.
 Computer programmers are those who write computer software.
One approach popular for requirements analysis is Use Case analysis.
 Programs were mostly entered using punched cards or paper tape.
 The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute.