However, readability is more than just programming style.
Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly.
Compiling takes the source code from a low-level programming language and converts it into machine code.
A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.
Normally the first step in debugging is to attempt to reproduce the problem.
However, Charles Babbage had already written his first program for the Analytical Engine in 1837.
Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.
Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses.
Transpiling on the other hand, takes the source-code from a high-level programming language and converts it into bytecode.
One approach popular for requirements analysis is Use Case analysis.
The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
 Different programming languages support different styles of programming (called programming paradigms).
 New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
There exist a lot of different approaches for each of those tasks.