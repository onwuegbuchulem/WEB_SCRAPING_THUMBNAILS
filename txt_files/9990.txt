Unreadable code often leads to bugs, inefficiencies, and duplicated code.
Relatedly, software engineering combines engineering techniques and principles with software development.
Their jobs usually involve:
 Although programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.
However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
Proficient programming thus usually requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference.
Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute.
Some of these factors include:
 The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.
It is usually easier to code in "high-level" languages than in "low-level" ones.
Also, specific user environment and usage history can make it difficult to reproduce the problem.
To produce machine code, the source code must either be compiled or transpiled.
 Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA).
Integrated development environments (IDEs) aim to integrate all such help.
 Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA).
For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input.